#!/usr/bin/env python3

########################################################################
# vanityhash, a hex hash fragment creation tool
# Copyright (C) 2010-2018 Ryan Finnie <ryan@finnie.org>
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version 2
# of the License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA
# 02110-1301, USA.
########################################################################

import os
import sys
import argparse
import hashlib
import struct
import multiprocessing
import queue
import time
import datetime
import codecs
import zlib
import copy
import random
import logging


__version__ = '2.0'


class VanityHash:
    """VanityHash class.

    Child subprocesses will have access to the instance of this class,
    forked from the master.
    """
    version = '2.0'

    # multiprocessing queue object
    queue = multiprocessing.Queue()
    # hashlib context
    ctx = None
    # Total number of hashes searched, across all children
    total_searched = 0
    # Total number of hashes found, across all children
    total_found = 0
    # In append mode, whether the binary result has been printed yet
    printed_append = False
    # Start time
    start_time = None
    # Next time to display the progress
    next_progress_time = None
    # Number of real workers to be used
    workers_real = 0
    # List of zero-indexed workers to be used
    workers_real_l = []
    # Total workers in the worker set
    workers_total = 0
    # Proporition of real to total workers
    workers_real_fraction = 0.0
    # Pack type of hash candidates
    pack_type = b'=L'

    # Argparse object
    args = None

    def main(self):
        """Main program loop."""
        # Parse getopts.
        self.args = self.parse_args()

        # Logging setup
        logging.basicConfig(
            format='%(message)s',
            level=(logging.ERROR if self.args.quiet else logging.INFO),
        )

        if self.args.list_digests:
            for digest in sorted(set(
                [x.lower() for x in ExtendedHashlib().algorithms_available]
            )):
                print(digest)
            return

        self.ctx = ExtendedHashlib().new(self.args.digest_type)
        # Pre-compute the largest integer to be tested.
        self.space_max = 0
        for i in range(0, self.args.bits):
            self.space_max += 2 ** i

        # Build a pack type based on the bits_pack size.
        if self.args.byte_order == 'little':
            self.pack_type = b'<'
        elif self.args.byte_order == 'big':
            self.pack_type = b'>'
        else:
            self.pack_type = b'='
        if self.args.bits_pack == 64:
            self.pack_type += b'Q'
        elif self.args.bits_pack == 32:
            self.pack_type += b'L'
        elif self.args.bits_pack == 16:
            self.pack_type += b'H'
        else:
            self.pack_type += b'B'

        # Read stdin data.
        self.read_data()

        if self.args.find_any_pos:
            logging.info('Searching for {} at any position in a {}-bit space.'.format(
                self.args.find, self.args.bits
            ))
        else:
            logging.info('Searching for {} at position {} in a {}-bit space.'.format(
                self.args.find, self.args.find_pos, self.args.bits
            ))

        self.start_time = datetime.datetime.now()

        # Spawn worker children.
        for i in self.workers_real_l:
            p = multiprocessing.Process(target=self.worker, args=(i,))
            p.name = 'Worker {}'.format(i + 1)
            p.start()

        if self.workers_total == self.workers_real:
            logging.info('Spawned {} worker{}.'.format(
                self.workers_real, ((self.workers_real != 1) and 's' or '')
            ))
        else:
            logging.info('Spawned {} of {} worker{} ({}).'.format(
                self.workers_real,
                self.workers_total,
                ((self.workers_total != 1) and 's' or ''),
                (','.join(str(x + 1) for x in self.workers_real_l)),
            ))

        # Loop through messages from children, occasionally reporting
        # hashing progress.
        self.next_progress_time = self.start_time + datetime.timedelta(seconds=self.args.progress_interval)
        while True:
            try:
                self.process_message()
                self.report_progress()
            except KeyboardInterrupt:
                logging.info('Stopping workers...')
                self.kill_children()

            if len(multiprocessing.active_children()) == 0:
                break

        if self.args.append and self.args.append_empty and not self.printed_append:
            sys.stdout.buffer.write(b'\x00' * int(self.args.bits_pack / 8))
            sys.stdout.buffer.flush()
            self.printed_append = True

        # Final statistics.
        logging.info('Search finished in {}, {} match{} found in {:0.02f}% of a {}-bit space.'.format(
            datetime.datetime.now() - self.start_time,
            self.total_found,
            ((self.total_found != 1) and 'es' or ''),
            ((self.total_searched - 1) / self.space_max * 100),
            self.args.bits,
        ))

    def worker(self, num_begin):
        """Process hash instructions in a subprocess.

        Note that the state of the class instance is the state at the
        time of the fork from the parent process.  Communication back to
        the parent is done by the queue object.
        """
        to_find = self.args.find
        to_find_len = len(to_find)
        find_pos = self.args.find_pos
        find_anypos = self.args.find_any_pos
        find_pos_end = find_pos + to_find_len

        # Start out with a group of 10,000 hashes.  This will be revised
        # to be approximately 2 seconds worth of hashes.
        last_report = time.time()
        report_i = 10000
        i = num_begin

        while i <= self.space_max:
            # Take into account multiple workers when determining when
            # to end the group.
            group_num_end = i + (report_i * self.workers_total)
            if group_num_end > self.space_max:
                group_num_end = self.space_max
            group_i_begin = i

            # The actual hash->test loop is as tight as possible, and
            # hence is duplicated a bit.
            if find_anypos:
                while i <= group_num_end:
                    ctxcopy = self.ctx.copy()
                    ctxcopy.update(struct.pack(self.pack_type, i))
                    hexdigest = ctxcopy.hexdigest()
                    if hexdigest.find(to_find) > -1:
                        self.queue.put(('FOUND', (hexdigest, i)))
                    i += self.workers_total
            else:
                while i <= group_num_end:
                    ctxcopy = self.ctx.copy()
                    ctxcopy.update(struct.pack(self.pack_type, i))
                    hexdigest = ctxcopy.hexdigest()
                    if hexdigest[find_pos:find_pos_end] == to_find:
                        self.queue.put(('FOUND', (hexdigest, i)))
                    i += self.workers_total

            # Figure out how many hashes were performed, and update the
            # parent.
            report_i = (i - group_i_begin) / self.workers_total
            self.queue.put(('PROGRESS', report_i))

            # Figure out how many hashes are needed to run for the next
            # ~2 seconds.
            now = time.time()
            next_report_i = int(2 * (report_i / (now - last_report)))
            last_report = now
            report_i = next_report_i

    def pretty_number(self, n, divisor=1000, rollover=1.0, format='{number:0.02f} {prefix}'):
        if divisor == 1024:
            prefixes = ['', 'Ki', 'Mi', 'Gi', 'Ti']
        else:
            prefixes = ['', 'K', 'M', 'G', 'T']
        ppos = 0
        max_ppos = len(prefixes) - 1
        while n >= (divisor * rollover):
            ppos = ppos + 1
            n = n / float(divisor)
            if ppos >= max_ppos:
                break
        return format.format(number=n, prefix=prefixes[ppos])

    def bytes_to_hex(self, b):
        """Return a hex representation of binary byte data."""
        return codecs.encode(b, 'hex_codec').decode('ascii')

    def parse_args(self, argv=None):
        """Parse user arguments."""
        if argv is None:
            argv = sys.argv

        parser = argparse.ArgumentParser(
            description='vanityhash ({})'.format(__version__),
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
            prog=os.path.basename(argv[0]),
        )

        parser.add_argument(
            '--version', '-V', action='version',
            version=__version__,
            help='report the program version',
        )

        parser.add_argument(
            'find', type=str, default=None, nargs='?',
            help='Hex string to search for',
        )

        parser.add_argument(
            '--bits', '-b', type=int, default=24,
            help='Search space, in bits',
        )
        parser.add_argument(
            '--workers', '-w', dest='workers_s', type=str, default='guess',
            help='Worker specification',
        )
        parser.add_argument(
            '--digest', '-d', dest='digest_type', type=str, default='md5',
            help='Hash digest type',
        )
        parser.add_argument(
            '--progress', '-s', dest='progress_interval', type=float, default=5.0,
            help='How often to display progress information, in seconds',
        )
        parser.add_argument(
            '--append', '-a', action='store_true',
            help='Whether to output the original data + the first result',
        )
        parser.add_argument(
            '--bits-pack', '-t', type=int, default=0,
            help='Total size containing the search space, in bits',
        )
        parser.add_argument(
            '--any_position', '-y', dest='find_any_pos', action='store_true',
            help='Whether to find the desired fragment anywhere in the hash',
        )
        parser.add_argument(
            '--quiet', '-q', action='store_true',
            help='Whether to display human-readable information to stderr',
        )
        parser.add_argument(
            '--position', '-p', dest='find_pos', type=int, default=0,
            help='Zero-indexed position within the hash to search',
        )
        parser.add_argument(
            '--byte-order', '-n', choices=['native', 'little', 'big'], default='native',
            help='Endianness of the built container',
        )
        parser.add_argument(
            '--append_empty', '-e', action='store_true',
            help='Where to add a zeroed pack in append mode, if no match is found',
        )
        parser.add_argument(
            '--list-digests', action='store_true',
            help='List available digests',
        )

        args = parser.parse_args(args=argv[1:])

        if args.digest_type == 'sha1alt':
            args.digest_type = 'sha1'
        if (args.bits < 1) or (args.bits > 64):
            parser.error('Search space must be 64 bits or less')

        # Generate the container size if not specified.
        if args.bits_pack == 0:
            args.bits_pack = 1
            while args.bits_pack < args.bits:
                args.bits_pack *= 2
            if args.bits_pack < 8:
                args.bits_pack = 8
        # Validate the container size.
        if (args.bits_pack < args.bits) or (args.bits_pack > 64):
            parser.error('Invalid bits-pack')
        # Make sure the container size is a power of 2.
        bits_pack_bytes = int(args.bits_pack / 8)
        if not (bits_pack_bytes & (bits_pack_bytes - 1)) == 0:
            parser.error('Invalid bits-pack')

        # Validate the desired hex fragment
        if args.find is None:
            if not args.list_digests:
                parser.error('Hex string required')
            # Fake value, won't be using it for list_digests
            args.find = 'ffffffff'
        for i in args.find:
            if i not in '0 1 2 3 4 5 6 7 8 9 a b c d e f'.split():
                parser.error('Invalid search hex string')

        # Build the worker options.
        if args.workers_s == 'guess':
            try:
                args.workers_s = str(multiprocessing.cpu_count())
            except NotImplementedError:
                args.workers_s = str(1)
        if args.workers_s.isdigit():
            # If a single number is given, the real and total workers
            # are the same.
            self.workers_total = int(args.workers_s)
            self.workers_real_l = range(self.workers_total)
        else:
            # If a specification is given, validate and build according
            # to the specification.
            try:
                (workert, workerx) = args.workers_s.split(':')
            except ValueError:
                parser.error('Invalid worker specification')
            self.workers_total = int(workert)
            for i in workerx.split(','):
                if not i.isdigit():
                    parser.error('Invalid worker specification')
                i = int(i)
                if (i > self.workers_total) or (i < 1):
                    parser.error('Invalid worker specification')
                if not (i - 1) in self.workers_real_l:
                    self.workers_real_l.append(i - 1)
                self.workers_real_l.sort()
        self.workers_real = len(self.workers_real_l)
        if (self.workers_total < 1) or (self.workers_real < 1):
            parser.error('Invalid number of workers')
        if self.workers_real > 128:
            parser.error('Cannot be more than 128 workers')
        self.workers_real_fraction = self.workers_real / self.workers_total

        # Test the hash type is valid.
        try:
            testctx = ExtendedHashlib().new(args.digest_type)
        except ValueError:
            parser.error('Invalid digest type')

        # Test the position specified is correct according to the given
        # hash type.
        hexdigestsize = testctx.digest_size * 2
        maxpos = hexdigestsize - len(args.find)
        if args.find_pos < 0:
            args.find_pos += hexdigestsize
        if args.find_pos > maxpos:
            parser.error('Pattern position {} goes beyond end of {} digest'.format(
                args.find_pos, args.digest_type.upper())
            )

        return args

    def process_message(self):
        """Parse a received child message."""
        try:
            msg = self.queue.get(block=True, timeout=1.0)
        except queue.Empty:
            return
        if msg[0] == 'PROGRESS':
            self.total_searched += msg[1]
        elif msg[0] == 'FOUND':
            (msgdigest, msgdata) = msg[1]
            msgdata = struct.pack(self.pack_type, msgdata)
            logging.info('Match found: 0x{} -> {} {}'.format(
                self.bytes_to_hex(msgdata), self.args.digest_type.upper(), msgdigest)
            )
            self.total_found += 1
            if self.args.append:
                if not self.printed_append:
                    sys.stdout.buffer.write(msgdata)
                    sys.stdout.buffer.flush()
                    self.printed_append = True
                    self.kill_children()
            else:
                sys.stdout.write('{} {}\n'.format(self.bytes_to_hex(msgdata), msgdigest))
                sys.stdout.flush()

    def read_data(self):
        """Read data from stdin and build the initial hash context."""
        self.ctx = ExtendedHashlib().new(self.args.digest_type)
        logging.info('Reading input data and adding to digest...')
        datalen = 0
        while True:
            buf = sys.stdin.buffer.read(1024)
            if not buf:
                break
            if self.args.append:
                sys.stdout.buffer.write(buf)
            datalen += len(buf)
            self.ctx.update(buf)
        if self.args.append:
            sys.stdout.buffer.flush()
        logging.info('Done.')

        origdigest = self.ctx.copy().hexdigest()
        logging.info('Original data: {} bytes, {} {}'.format(datalen, self.args.digest_type.upper(), origdigest))

    def report_progress(self):
        """Occasionally output progress statistics."""
        now = datetime.datetime.now()
        if not now > self.next_progress_time:
            return
        elapsed = now - self.start_time
        percent = self.total_searched / (self.space_max * self.workers_real_fraction) * 100
        if self.total_searched > 0:
            hashes_per_sec = self.total_searched / elapsed.total_seconds()
            estimated_time = datetime.timedelta(
                seconds=((self.space_max * self.workers_real_fraction) / hashes_per_sec)
            )
            remaining = datetime.timedelta(
                seconds=((self.space_max * self.workers_real_fraction - self.total_searched) / hashes_per_sec)
            )
            remaining = estimated_time - elapsed
            logging.info('{:0.02f}% searched, ~{} remaining of ~{}, {}'.format(
                percent,
                remaining - datetime.timedelta(microseconds=remaining.microseconds),
                estimated_time - datetime.timedelta(microseconds=estimated_time.microseconds),
                self.pretty_number(hashes_per_sec, format='{number:0.02f} {prefix}hash/s'),
            ))
        else:
            logging.info('{:0.02f}% searched...'.format(percent))
        self.next_progress_time = now + datetime.timedelta(seconds=self.args.progress_interval)

    def kill_children(self):
        """Kill all child subprocesses."""
        for child_process in multiprocessing.active_children():
            child_process.terminate()


class HashlibCRC32:
    """hashlib-compatible CRC32."""
    _crc = 0
    name = 'crc32'
    digestsize = 4
    digest_size = 4
    block_size = 1

    def copy(self):
        return copy.copy(self)

    def update(self, data):
        self._crc = zlib.crc32(data, self._crc)

    def digest(self):
        return struct.pack(b'>I', (self._crc & 0xffffffff))

    def hexdigest(self):
        return codecs.encode(self.digest(), 'hex_codec')


class HashlibAdler32:
    """hashlib-compatible Adler-32."""
    _checksum = 1
    name = 'adler32'
    digestsize = 4
    digest_size = 4
    block_size = 1

    def copy(self):
        return copy.copy(self)

    def update(self, data):
        self._checksum = zlib.adler32(data, self._checksum)

    def digest(self):
        return struct.pack(b'>I', (self._checksum & 0xffffffff))

    def hexdigest(self):
        return codecs.encode(self.digest(), 'hex_codec')


class HashlibRandom:
    """hashlib-compatible dummy random module."""
    _checksum = 0
    name = 'random'
    digestsize = 32
    digest_size = 32
    block_size = 64

    def __init__(self, digest_size=32, block_size=64):
        self.digestsize = digest_size
        self.digest_size = digest_size
        self.block_size = block_size
        self.update('')

    def copy(self):
        return copy.copy(self)

    def update(self, data):
        self._checksum = bytes([random.randint(0, 255) for x in range(self.digest_size)])

    def digest(self):
        return self._checksum

    def hexdigest(self):
        return codecs.encode(self.digest(), 'hex_codec')


class HashlibNull:
    """hashlib-compatible dummy null module."""
    name = 'null'
    _checksum = b''
    _checksum_hex = ''
    digestsize = 32
    digest_size = 32
    block_size = 64

    def __init__(self, digest_size=32, block_size=64):
        self.digestsize = digest_size
        self.digest_size = digest_size
        self.block_size = block_size
        self._checksum = bytes(digest_size)
        self._checksum_hex = codecs.encode(self._checksum, 'hex_codec')

    def copy(self):
        # Cheating here in the name of speed
        return self

    def update(self, data):
        pass

    def digest(self):
        return self._checksum

    def hexdigest(self):
        return self._checksum_hex


class ExtendedHashlib:
    """hashlib-compatible extension system."""
    extended_algorithms = {
        'random': HashlibRandom,
        'null': HashlibNull,
        'crc32': HashlibCRC32,
        'adler32': HashlibAdler32,
    }

    def __init__(self):
        # https://github.com/rfinnie/vanityhash/issues/1
        try:
            import xxhash
            self.extended_algorithms['xxh32'] = xxhash.xxh32
            self.extended_algorithms['xxh64'] = xxhash.xxh64
        except (ImportError, AttributeError):
            pass

        self.algorithms_available = set(
            tuple(hashlib.algorithms_available) +
            tuple(self.extended_algorithms.keys()))
        self._hashlib_algorithms = hashlib.algorithms_available
        self.algorithms_guaranteed = hashlib.algorithms_guaranteed
        for algo in hashlib.algorithms_guaranteed:
            vars(self)[algo] = getattr(hashlib, algo)

    def new(self, algo, **kwargs):
        if algo in self.extended_algorithms:
            return self.extended_algorithms[algo](**kwargs)
        else:
            return hashlib.new(algo, **kwargs)


if __name__ == '__main__':
    vh = VanityHash()
    vh.main()
